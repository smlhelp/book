<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Work and Span - SML Help</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="A resource for learning SML">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">Welcome!</a></li><li class="chapter-item expanded "><a href="../start/index.html"><strong aria-hidden="true">1.</strong> Getting Started</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../start/install.html"><strong aria-hidden="true">1.1.</strong> Install & run SML</a></li><li class="chapter-item expanded "><a href="../start/syntax.html"><strong aria-hidden="true">1.2.</strong> Syntax Cheatsheet</a></li><li class="chapter-item expanded "><a href="../start/common.html"><strong aria-hidden="true">1.3.</strong> Common Tasks</a></li></ol></li><li class="chapter-item expanded "><a href="../types/index.html"><strong aria-hidden="true">2.</strong> Types & Signatures</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../types/type.html"><strong aria-hidden="true">2.1.</strong> Types</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../types/bool.html"><strong aria-hidden="true">2.1.1.</strong> bool</a></li><li class="chapter-item expanded "><a href="../types/int.html"><strong aria-hidden="true">2.1.2.</strong> int</a></li><li class="chapter-item expanded "><a href="../types/real.html"><strong aria-hidden="true">2.1.3.</strong> real</a></li><li class="chapter-item expanded "><a href="../types/string.html"><strong aria-hidden="true">2.1.4.</strong> string</a></li><li class="chapter-item expanded "><a href="../types/function.html"><strong aria-hidden="true">2.1.5.</strong> function types</a></li><li class="chapter-item expanded "><a href="../types/list.html"><strong aria-hidden="true">2.1.6.</strong> list</a></li><li class="chapter-item expanded "><a href="../types/options.html"><strong aria-hidden="true">2.1.7.</strong> options</a></li></ol></li><li class="chapter-item expanded "><a href="../types/sig.html"><strong aria-hidden="true">2.2.</strong> Signatures</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../types/basis.html"><strong aria-hidden="true">2.2.1.</strong> Basis Library Documentation</a></li><li class="chapter-item expanded "><a href="../types/aux-lib.html"><strong aria-hidden="true">2.2.2.</strong> Auxiliary Library Documentation</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../debugging/index.html"><strong aria-hidden="true">3.</strong> Debugging</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../debugging/errors.html"><strong aria-hidden="true">3.1.</strong> Common Errors</a></li><li class="chapter-item expanded "><a href="../debugging/hints.html"><strong aria-hidden="true">3.2.</strong> Debugging Hints</a></li></ol></li><li class="chapter-item expanded "><a href="../concepts/index.html"><strong aria-hidden="true">4.</strong> Concepts</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../concepts/basic.html"><strong aria-hidden="true">4.1.</strong> Basics</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../concepts/eval.html"><strong aria-hidden="true">4.1.1.</strong> Evaluation</a></li><li class="chapter-item expanded "><a href="../concepts/eeq.html"><strong aria-hidden="true">4.1.2.</strong> Extensional Equivalence</a></li><li class="chapter-item expanded "><a href="../concepts/patternmatch.html"><strong aria-hidden="true">4.1.3.</strong> Pattern Matching</a></li></ol></li><li class="chapter-item expanded "><a href="../concepts/recind.html"><strong aria-hidden="true">4.2.</strong> Recursion and Induction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../concepts/induct.html"><strong aria-hidden="true">4.2.1.</strong> Inductive Proofs</a></li><li class="chapter-item expanded "><a href="../concepts/rec.html"><strong aria-hidden="true">4.2.2.</strong> Recursion</a></li><li class="chapter-item expanded "><a href="../concepts/tail.html"><strong aria-hidden="true">4.2.3.</strong> Tail Recursion</a></li></ol></li><li class="chapter-item expanded "><a href="../concepts/analysis.html"><strong aria-hidden="true">4.3.</strong> Asymptotic Analysis</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../concepts/workspan.html" class="active"><strong aria-hidden="true">4.3.1.</strong> Work and Span</a></li></ol></li><li class="chapter-item expanded "><a href="../concepts/datatypes.html"><strong aria-hidden="true">4.4.</strong> Datatypes</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../concepts/poly.html"><strong aria-hidden="true">4.4.1.</strong> Parametric Polymorphism</a></li></ol></li><li class="chapter-item expanded "><a href="../concepts/hofs.html"><strong aria-hidden="true">4.5.</strong> Higher Order Functions</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../concepts/curry.html"><strong aria-hidden="true">4.5.1.</strong> Currying and Staging</a></li><li class="chapter-item expanded "><a href="../concepts/common.html"><strong aria-hidden="true">4.5.2.</strong> Common HOFs and Partial Evaluation</a></li></ol></li><li class="chapter-item expanded "><a href="../concepts/control.html"><strong aria-hidden="true">4.6.</strong> Control Flow</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../concepts/cps.html"><strong aria-hidden="true">4.6.1.</strong> Continuation Passing Style</a></li><li class="chapter-item expanded "><a href="../concepts/exn.html"><strong aria-hidden="true">4.6.2.</strong> Exceptions</a></li></ol></li><li class="chapter-item expanded "><a href="../concepts/mods.html"><strong aria-hidden="true">4.7.</strong> Modules</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../concepts/mods.html"><strong aria-hidden="true">4.7.1.</strong> Basics and Ascription</a></li><li class="chapter-item expanded "><a href="../concepts/functors.html"><strong aria-hidden="true">4.7.2.</strong> Functors</a></li></ol></li><li class="chapter-item expanded "><a href="../concepts/apps.html"><strong aria-hidden="true">4.8.</strong> Applications</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../concepts/sequences.html"><strong aria-hidden="true">4.8.1.</strong> Sequences</a></li></ol></li><li class="chapter-item expanded "><a href="../concepts/lazy.html"><strong aria-hidden="true">4.9.</strong> Lazy Evaluation</a></li><li class="chapter-item expanded "><a href="../concepts/imperative.html"><strong aria-hidden="true">4.10.</strong> Imperative</a></li></ol></li><li class="chapter-item expanded "><a href="../examples/index.html"><strong aria-hidden="true">5.</strong> Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../examples/basics.html"><strong aria-hidden="true">5.1.</strong> Basics</a></li><li class="chapter-item expanded "><a href="../examples/recursion.html"><strong aria-hidden="true">5.2.</strong> Recursion & Induction</a></li></ol></li><li class="chapter-item expanded "><a href="../about.html">About</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">SML Help</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <style>
.aligncenter {
    text-align: center;
}
</style>
<h1 id="work-and-span"><a class="header" href="#work-and-span">Work and Span</a></h1>
<p><em>By Aditi Gupta and Brandon Wu, May 2020</em>. <em>Revised September 2020</em></p>
<p>We will now turn towards a more robust notion of <em>work</em> and <em>span</em> that let us
analyze our conception of asymptotic runtime more effectively. It is still
dependent on asymptotic analysis, but merely involves being more involved with
how we go about generating the asymptotic bound for a function from the code
itself. Additionally, we will not only analyze the approximate <em>number of steps</em>
of the program (which corresponds to the <em>runtime</em> of the program, given
sequential execution), but also the approximate <em>longest chain of dependencies</em>
that exists in the program, assuming that computations can be run in parallel.
We will elaborate more on this idea in this chapter.</p>
<h2 id="parallel-computing"><a class="header" href="#parallel-computing">Parallel Computing</a></h2>
<p>It is intuitive to view things occurring sequentially. Whether it is reading a
tutorial, writing a list of instructions, or making a plan for the future,
sequential actions are very easy to think about. Even programs are written in a
stepwise manner, with computations happening one after the other in a prescribed
way. It seems to be in our nature to impose some kind of order on a list of
actions.</p>
<p>Despite that, however, sequential evaluation is not always the most <em>efficient</em>.
Sequential evaluation introduces <em>dependencies</em> where other subtasks cannot be
started until we have finished the current subtask, which has the effect of
potentially inducing wait times where none exist. For instance, if your plan is
to do the laundry and your homework, it might not be the most time-efficient to
wait until the washer is done before get started on your work. There is no
dependency between laundry and homework - there is no logical reason why you
should have to wait for one before the other, so you could do them both at the
<em>same time</em>, or in <em>parallel</em>.</p>
<p>Parallel computing is a principle that is becoming more and more important as
time goes on. Computers now more frequently have multiple cores in their
processors, which means that tasks can be subdivided and assigned out to
independent acting agents. </p>
<p>The benefits of doing so are clear. Suppose that we are stacking a shelf with
merchandise. If the shelf is tall, this may take us a while - roughly linear in
the height of the shelf, we can imagine (supposing we have an infinite stock of
items, and that climbing a ladder somehow isn't a factor). If we had a person to
dedicate to each shelf, however, then we could stock the shelves in &quot;constant&quot;
time - independent of the number of shelves that there actually are. This will
be a driving idea behind how we look at parallelism.</p>
<p>While we will not delve into the implementation details of parallel computing
(which is more apt for a systems perspective), we will perform basic analysis of
asymptotic complexity based on that premise. These take the form of <em>work</em> and
<em>span</em>, fundamental ideas that will drive the next section.</p>
<h2 id="theory"><a class="header" href="#theory">Theory</a></h2>
<p>First, let us consider what we will term a <em>task dependency graph</em>. This is not
so important of a concept to memorize, but it will help in conceptualizing work
and span. A task dependency graph is a directed acyclic graph (that is, a graph
whose edges are one-way, and there exist no loops) that represents the
dependencies when trying to perform a set of tasks. Each node is a task,
labelled with the time that it takes to execute it (which is a singular unit,
unable to be reduced otherwise), as well as edges that represent the
dependencies in the graph. Any task cannot be started until all of the tasks
that have edges directed towards it are finished - that is, all of a task's
inbound edges denote its prerequisites.</p>
<p>With this knowledge, we will be able to define what we mean by work and span.</p>
<blockquote>
<p><strong>[Work]</strong> The <em>work</em> of a computation denotes the number of steps it takes to
run, assuming access to only a single processor. Work thus represents the
worst-case sequential evaluation time, and can be upper bounded with
asymptotic analysis.</p>
</blockquote>
<blockquote>
<p><strong>[Span]</strong> The <em>span</em> of a computation denotes the number of steps that it
takes to run, assuming access to infinitely many processors that allow us to
run tasks in parallel. Span thus represents the worst-case parallel evaluation
time, and can be upper bounded with asymptotic analysis.</p>
</blockquote>
<p>What we find is that work directly corresponds to our previous intuition of the
complexity of a computation, since our previous analyses have always been
sequential. Span, however, is not quite as easy to eyeball. Now, we are really
looking for the <em>longest chain of dependencies</em>, that is, the longest sequence
of tasks that <em>must</em> be run sequentially, since everything else can be run
concurrently. Infinitely many processors, while obviously unrealistic, helps to
simplify our analysis, and provides us a &quot;target&quot; for the efficiency of a
<em>fully</em> parallel algorithm.</p>
<p>We illustrate these concepts with the following graph.</p>
<figure class="aligncenter">
    <img src="../assets/graphTransCropped.png" alt="Process Graph" width="1000"/>
    <figcaption><b>Fig 1.</b> Task dependency graph illustrating dependencies between tasks and task durations</figcaption>
</figure>
<p>So in this example, the work of our graph would be \( 1+3+6+2+5+9+3+3+10 = 42 \),
since with a single processor, the dependencies don't really matter to us. We
have no choice but to complete every task, and the precise order doesn't matter.
That isn't to say that we can execute the tasks in any order, that plainly isn't
true - we simply mean that there is no order that can change what our runtime
is.</p>
<p>On the other hand, for span we must consider the length of the <em>longest path</em>.
The span of this graph would thus be \( 1 + 3 + 6 + 9 + 10 = 29 \), since that is
the longest path. Even being able to execute everything else in a parallel way,
we cannot avoid the fact that these nodes must follow one after the other. This
path is thus the limiting factor in our runtime - ultimately it constrains the
amount of time that we expend.</p>
<p>Task dependency graphs are a concept that we discuss purely for theoretically
being able to understand the idea of work and span. We will look at examples in
terms of actual SML code in the next section, which will be primarily where we
do our work/span analysis.</p>
<h2 id="workspan-analysis-of-code"><a class="header" href="#workspan-analysis-of-code">Work/Span Analysis of Code</a></h2>
<p>The previous example was rather contrived. For one thing, it is <em>prespecified</em> -
we already knew all of the tasks that there were, along with its dependencies
and task times. As such, we could compute a simple, numerical answer. This will
likely not be the case. We are interested in work/span analysis of <em>algorithms</em>,
which will yield us <em>another</em> function - one describing the runtime complexity
of the algorithm as a function of some notion of input size.</p>
<p>For recursive functions, work/span analysis is very easy to do. We characterize
it in terms of <em>recurrence relations</em>, which are themselves recursive functions
describing the work or span of some code. Then, we simply solve for the closed
form of the recurrence relation and estimate a Big-O bound to arrive at our
desired complexity.</p>
<p>Consider the following example:</p>
<pre><code class="language-sml">fun length ([] : int list) : int = 0
  | length (x::xs : int list) : int = 1 + length xs
</code></pre>
<p>The first step to determining the work and span of such a function is to write a
recurrence relation. These steps are explicit - the code should determine the
recurrence relation, and the recurrence relation should determine the Big-O
bound. We will first analyze this function's work complexity, then move on to
span.</p>
<p>First, we should fix some notion of input size. This will differ based on what
our recurrence is recursing on, but in this example it seems to be the size of
the input list. Note that this follows directly from the code - if this were the
factorial function, we may say that the recurrence is in terms of the value of
the input, and as we will later see, if the input was a tree, we may write the
recurrence in terms of the number of nodes in the tree.</p>
<p>So we can write the following recurrence for work. We will explain soon what
exactly it means, and how to arrive at it:</p>
<center> \( W_{length}(n) = c_0 + W_{length}(n-1) \) </center>
<center> \( W_{length}(0) = c_1 \) </center>
<p>This recurrence is made of two parts - the recursive case and the base case. The
first equation for \( W_{length}(n) \) simply denotes what the work for an input
size of \( n \) should be - defined recursively. The second equation for
\( W_{length}(0) \) defines what the work for an input size of \( 0 \) should be.
This directly corresponds to our code, which has two clauses for a list of
length \( 0 \) (that being <code>[]</code>), and for the general case. This is an important
observation to make, that the recurrence follows directly from the code.</p>
<p>The recursive case says that, for an input of size \( n \), the work done is \( c_0 + W_{length}(n-1) \). 
Here, \( c_0 \) denotes <em>some</em> constant. This is supposed to correspond to the recursive case of 
the function, and if we look at it, we have a recursive call <code>length xs</code>, as well as some 
other work of adding one. Adding one, being an arithmetic operation, is a constant-time 
process, meaning that it takes a non-zero constant amount of time. This is what \( c_0 \) is supposed to
represent - the constant amount of non-recursive work that must be done, after
the recursive call has finished. It is not important what \( c_0 \) is, just that
it is some unspecified amount of work that is not a function of \( n \).</p>
<p>Conversely, \( W_{length}(n-1) \) represents exactly the amount of work done by
the recursive call, since it is literally defined to be the amount of work done
by an input of size \( n-1 \), which is exactly what happens when we call <code>length xs</code>, where <code>xs</code> has length \( n-1 \).</p>
<p><strong>NOTE:</strong> Even if we did not have the addition operation, we would still have
\( c_0 \). This is because merely entering the function and figuring out which
case to execute takes some non-zero amount of work - it is impossible to run the
recursive call perfectly with no other time expense. As such, we would see
exactly the same recurrence even if the recursive case was <code>length (x::xs : int list) : int = length xs</code> (which would also be a very bad length function).</p>
<p>For the base case, we have that \( W_{length}(0) = c_1 \), since in the base case
we just return 0. This has a constant amount of work associated with it, as
argued previously, so we use the constant \( c_1 \) to denote that, since the
amount of work is likely not the same constant as that in the recursive case,
when adding 1. </p>
<p>So this is how we arrive at the work recurrence for <code>length</code>. We will now turn
to the span recurrence, which we obtain as:</p>
<center> \( S_{length}(n) = c_0 + S_{length}(n-1) \) </center>
<center> \( S_{length}(0) = c_1 \) </center>
<p>Note that the span recurrence is exactly the same as the work recurrence. This
should make sense, because there is no opportunity for parallelism in the
<code>length</code> function - we can only pop off elements one by one from the list. In
the recursive case, we must wait for the result of the recursive call on <code>xs</code>,
which means we unavoidably must expend the span of \( S_{length}(n-1) \) -
additionally, we have a data dependency. We cannot execute the addition in <code>1 + length xs</code> until we obtain the result for <code>length xs</code>, which means that we must
sum the time it takes to compute <code>length xs</code> (that being \( S_{length}(n-1) \))
and the time it takes to carry out the addition operation (that being \( c_1 \)).</p>
<p>Now we will begin the task of actually solving the recurrence. They are the same recurrence, so without loss of generality we will solve just the work recurrence.</p>
<p>We know that it has the form of \( W_{length}(n) = c_0 + W_{length}(n-1) \), and
eventually reaches a base case at \( W_{length}(0) = c_1 \). We can &quot;unroll&quot; the
recurrence a few times to see if we can see a pattern, and then arrive at our
answer.</p>
<p>So we start out with \( W_{length}(n) = c_0 + W_{length}(n-1) \), but if we invoke
the definition of \( W_{length}(n-1) \), we can produce \( c_0 + c_0 +
W_{length}(n-2) \), since \( W_{length}(n-1) = c_0 + W_{length}(n-2) \). By doing
the same for \( W_{length}(n-2) \), we get \( c_0 + c_0 + c_0 + W_{length}(n-3) \).
It seems we've hit upon a pattern - each time we &quot;unroll&quot; the definition of
\( W_{length}(n) \), for progressively lower \( n \), we get another \( c_0 \) term
back out. Then, we know that the recurrence should eventually solve to:</p>
<center> \( W_{length}(n) = (\sum_{i=1}^n c_0) + c_1 \) </center>
<p>We will usually omit the \( c_1 \), since it does not matter asymptotically. Then, clearly this is equivalent to \( nc_0 + c_1 \). We see that this closed-form solution is linear in \( n \) - so 
then we have that the work and span of this function is in \( O(n) \), which is consistent with what we would expect if we had &quot;eyeballed&quot; it.</p>
<h2 id="workspan-analysis-trees"><a class="header" href="#workspan-analysis-trees">Work/Span Analysis: Trees</a></h2>
<p>First, we will discuss the definition of a binary tree in SML:</p>
<pre><code class="language-sml">datatype tree = Empty 
              | Node of tree * int * tree
</code></pre>
<p>This denotes that a tree is either the constant constructor <code>Empty</code> denoting the empty tree, or a <code>Node</code> that contains an integer value, as well as two <code>tree</code> children, that can themselves be <code>Node</code>s or <code>Empty</code>.</p>
<figure class="aligncenter">
    <img src="../assets/treeTrans.png" alt="Tree" width="500"/>
    <figcaption><b>Fig 2.</b> Sample binary tree</figcaption>
</figure>
<p>So for instance, we may represent the above tree with <code>Node(Node(Node(Empty, 4, Empty), 3, Empty), 1, Node(Empty, 2, Empty))</code>. Put more fancily:</p>
<pre><code class="language-sml">Node(
    Node(
        Node(
            Empty,
            4,
            Empty
        ),
        3,
        Empty
    ),
    1,
    Node(
        Empty,
        2,
        Empty
    )
)
</code></pre>
<p>Now we will analyze the complexity of finding the size of a tree. Consider the
following implementation for doing so:</p>
<pre><code class="language-sml">fun size (Empty : tree) : int = 0
  | size (Node (L,x,R) : tree) : int = size L + 1 + size R
</code></pre>
<p>First convince yourself that it actually works. It simply recursively finds the
size of the left and right tree, then adds one for the node that it is currently
at. In the empty case, we consider the empty tree to have a size of 0.</p>
<p>The major difference between this function and the previous <code>length</code> function
was that <code>length</code> had one recursive call - <code>size</code> has two. We will need to
reflect this change when we write our recurrences. Additionally, we need a new
variable for our recurrence - we no longer have a list whose length we can
induct on. A similar analogue will be \( n \), the number of nodes in the tree, so
we will take that as our recurrence variable. We will focus first on work.</p>
<p>We will obtain the following work recurrence:</p>
<center> \( W_{size}(n) = c_0 + W_{size}(n_l) + W_{size}(n_r) \) </center>
<center> \( W_{size}(0) = c_1 \) </center>
<p>where we define the number of nodes in the tree \( n = 1 + n_l + n_r \), and
\( n_l \) and \( n_r \) denote the number of nodes in the left and right subtree,
respectively. This follows similarly to our recurrence for <code>length</code> in the
previous part, where <code>c_0</code> is just some constant amount of work that we
necessarily have to do, and the two \( W_{size} \) calls are from the two
recursive calls we make to <code>L</code> and <code>R</code>. </p>
<p>Now, we don't know precisely how big \( n_l \) and \( n_r \) are, with respect to
\( n \). This makes our analysis a little more tricky, but essentially all we need
to do is think of the <em>worst case</em>, as we are interested in the worst-case
asymptotic complexity of this function. For work, however, there is no
worst-case - no matter how the tree is structured, we must visit every node
once, doing a constant amount of work each time. So we should obtain, in the
end, \( W_{size}(n) = nc_0 + c_1 \), which we know is \( O(n) \). So in this case,
we didn't have to think about the structure of the tree. In the next section, it
will matter.</p>
<h2 id="workspan-analysis-balanced-vs-unbalanced-trees"><a class="header" href="#workspan-analysis-balanced-vs-unbalanced-trees">Work/Span Analysis: Balanced vs Unbalanced Trees</a></h2>
<p>We will revisit the same example, except from the perspective of span.</p>
<p>The important point to note is that, now, we have two separate recursive calls
that are happening in the recursive call of <code>size</code>. These recursive calls have
no data dependency - neither running depends on the other. This means that they
can be run in <em>parallel</em>, which means that the total span that we compute should
just be the max over both. This is because we can imagine that both of them lead
to different &quot;paths&quot; in our task-dependency graph - we are only interested in
the maximum-length path. So we will run both results, and whichever one takes
longer to return an answer to us is the &quot;limiting reagent&quot; of our computation.</p>
<p>So we will write the span recurrence as follows:</p>
<center> \( S_{size}(n) = c_0 + max(S_{size}(n_l), S_{size}(n_r)) \) </center>
<center> \( S_{size}(0) = c_1 \) </center>
<p>Now note that we are taking the max over the two recursive calls. Now, we cannot
handwave the structure of the tree like we did in the previous part - if one
path is significantly longer than the other, then it will stall the computation
for longer. We still must visit every node, but some of them can occur in
parallel.</p>
<p>We will consider the first case - if we have an unbalanced tree. Suppose that
the tree is heavily unbalanced - akin to a (diagonal) linked list. Without loss
of generality, let it be &quot;pointing&quot; to the left. Then, \( n_l = n - 1 \), and
\( n_r = 0 \). Then, the max over both recursive calls should clearly be that of
\( S_{size}(n-1) \), since it has to compute the size of a larger tree.</p>
<p>So we can update our recurrence and obtain:</p>
<center> \( S_{size}(n) = c_0 + S_{size}(n-1) \) </center>
<center> \( S_{size}(0) = c_1 \) </center>
<p>This recurrence is exactly the same as that of <code>length</code>, so we know that we will
get that \( S(n) \in O(n) \). This should make sense intuitively, since the depth
of the tree is \( n \), and there are dependencies between each level - we cannot
go to the next level until we are done with the current one. So we cannot avoid
having to visit every level sequentially, which results in \( O(n) \) span.</p>
<p>Now, what if we consider a balanced tree? Well, the balanced case would be if
the number of nodes in the left and right subtrees are roughly equal - that is,
\( n_l = n_r = \frac{n}{2} \). We will consider them exactly equal to simplify our
analysis, but we will obtain the same asymptotic answer. Then, we know that the
maximum is just any one of them, since they will have the same span.</p>
<p>So we can update our recurrence and obtain:</p>
<center> \( S_{size}(n) = c_0 + S_{size}(\frac{n}{2}) \) </center>
<center> \( S_{size}(0) = c_1 \) </center>
<p>This is slightly different than our <code>length</code> recurrence. We will try unrolling
to make sense of this recurrence.</p>
<p>We have that \( S_{size}(n) = c_0 + S_{size}(\frac{n}{2}) \). Plugging in the
recursive definition of \( S_{size}(\frac{n}{2}) \), we get that this expands to
\( c_0 + c_0 + S_{size}(\frac{n}{4}) \), which by the same trick expands to \( c_0+ c_0 + c_0 + S_{size}(\frac{n}{8}) \), 
and so on and so forth. We note that we
are dividing the number of nodes by 2 each time - and we know that we can divide
\( n \) by two roughly \( \log_2(n) \) times. So in total, we can solve the
summation of \( S_{size}(n) \) as \( S_{size} = (\sum_{i=1}^{\log_2(n)} c_0) +
c_1 \).</p>
<p>So then this simplifies to \( S_{size}(n) = \log_2(n)c_0 + c_1 \). This is a
logarithmic function of \( n \), so we get that the span of <code>size</code> is in \( O(\log
n) \). Thus, we obtain a different span for balanced trees versus unbalanced
trees - balanced trees are more efficient and parallelism-friendly.</p>
<h2 id="workspan-analysis-size-dependent-operations"><a class="header" href="#workspan-analysis-size-dependent-operations">Work/Span Analysis: Size-dependent Operations</a></h2>
<p>In these past two examples, we have only seen examples that did a constant
amount of non-recursive work. We will now analyze a function that does
non-recursive work that is a function of the input size \( n \). This will result
in different kinds of recurrences. First, however, we will digress briefly to motivate the example that we will analyze.</p>
<blockquote>
<p><strong>[Case Study: Tree Traversal]</strong></p>
<p>When analyzing trees, it is often prudent to utilize <em>tree traversal</em>, or a
systematic way of enumerating the elements in a tree. There are multiple
different ways to do this, depending on what your intentions are - a few namely
being preorder, inorder, and postorder traversal.</p>
<p>With these different methods of traversal, we can turn a tree into a different
kind of ordered data structure, such as a list or sequence. This can come in
handy when we desire future fast access to any arbitrarily ranked node in the
tree, or if we want to convert it for purposes of printing, for instance.</p>
<p>Each traversal is characterized by a certain &quot;strategy&quot; of traversal, depending
on how it ranks the three possible directions that it can go - root, left, and
right. Inorder traversal, for instance, is characterized by left-root-right
prioritization - this means that it goes left first, and if it can't go left,
then it visits the root node, and otherwise it goes right. Note that this does
not mean that it visits the root of the left subtree first - it simply reruns
the same process on the entire left subtree. No matter what the traversal
strategy is, a node is never actually visited until the &quot;root&quot; action is taken.
Preorder traversal is root-left-right, and postorder is left-right-root.
Examples of preorder and inorder traversals (the most common you will see in
this book) are below.</p>
</blockquote>
<figure class="aligncenter">
    <img src="../assets/traversals.png" alt="traversal" width="650"/>
    <figcaption><b>Fig 3.</b> An example of a preorder traversal (left) and inorder traversal (right) of a binary tree, with visited nodes labeled in ascending order </figcaption>
</figure>
<blockquote>
<p>Tree traversals can also come in handy when generating different notations for
mathematical expressions when represented in the form of an <em>binary expression
tree</em>, which has nodes that consist of either a <em>numeric constant</em>, which has
no children, a <em>unary operation</em> with a single child, or a <em>binary operation</em>
with two children. For instance, a binary expression tree for the mathematical
expression \( (4-1) * 2 \) is shown below.</p>
</blockquote>
<figure class="aligncenter">
    <img src="../assets/optree.png" alt="optree" width="500"/>
    <figcaption><b>Fig 4.</b> A binary expression tree for the expression \( (4-1) * 2 \)</figcaption>
</figure>
<blockquote>
<p>With inorder traversal of this expression tree, we can generate the constants
and symbols in exactly the same order as \( (4-1) * 2 \), which is how we would
normally interpret it. Preorder and postorder traversal, however, result in an
interesting interpretation - what is known as <em>prefix</em> (or <em>Polish</em>) and <em>postfix</em> (or <em>Reverse Polish</em>) notation.</p>
<p>In prefix notation, by using preorder traversal, we obtain the expression \( * - 4 1 2 \), which is how we would interpret the same expression if all of our operators appeared before their operands. Similarly, with postorder traversal, we obtain the expression \( 4 1 - 2 * \) in postfix notation. Prefix and postfix notation have significance in their lack of ambiguity - while infix notation is easy for humans to read, it requires parentheses sometimes to denote how operator precedence takes place. Prefix and postfix notation have no such flaw - they are unambiguous in how operations take place. In programming language interpreters, such notations are sometimes used to represent mathematical expressions.</p>
</blockquote>
<p>Such a digression serves as motivation for the next function that we will analyze - which is writing the preorder traversal of a tree in SML. The code looks like this:</p>
<pre><code class="language-sml">fun preord (Empty : tree) : int list = []
  | preord (Node(L, x, R) : tree) : int list = x :: (preord L @ preord R)
</code></pre>
<p>We can readily see that this follows the root-left-right order that we specified
earlier for preorder traversal. Recall that <code>@</code> is the function for list
concatenation, and has a complexity of \( O(n_l) \) in \( n_l \), the size of the
left input list. Thus, as stated before, this function has a non-constant amount
of work at each recursive call - we must evaluate <code>@</code> of the result of <code>preord L</code> and <code>preord R</code>, which is a function of \( n \), the number of nodes in the
tree.</p>
<p>We will analyze only the balanced case for this function. We invite the reader
to think about the unbalanced case on their own.</p>
<p>For the recursive case, we know that \( W_{preord}(n) \) will take the form of 
\( c_0 + W_@(n_l) + W_{preord}(n_l) + W_{preord}(n_r) \). By our balanced assumption, 
we know \( n_l = n_r = \frac{n}{2} \), so we can write our work recurrence as:</p>
<center> \( W_{preord}(n) = c_0 + \frac{n}{2}c_1 + 2W_{preord}(\frac{n}{2}) \) </center>
<center> \( W_{preord}(0) = c_2 \) </center>
<p>Note that the term \( W_@(n_l) \) is a recurrence in terms of \( n \), the size of the left list given as input to <code>@</code>. 
Since we know that the work complexity of <code>@</code> is \( O(n) \), we can replace \( W_@(\frac{n}{2}) \) with \( \frac{n}{2}c_1 \), 
which is simply some constant \( c_1 \), scaled by a linear factor of the input \( \frac{n}{2} \). This is how we will generally 
deal with analyzing the complexity of functions that make use of helper functions.</p>
<p>We will make use of a new method to solve this recurrence - the Tree Method.</p>
<blockquote>
<p><strong>[Tree Method]</strong> The <em>tree method</em> is a method for solving recurrences of certain recurrences that sum to the same quantity across levels, and usually have multiple recursive calls. In essence, if each level has the same amount of computation, then the recurrence solves to the (number of levels) * (amount at each level).</p>
</blockquote>
<p>The below diagram illustrates the Tree Method.</p>
<figure class="aligncenter">
    <img src="../assets/treemethodTrans.png" alt="Tree method" width="1200"/>
    <figcaption><b>Fig 5.</b> An illustration of the Tree Method for the recurrence of preord.</figcaption>
</figure>
<p>We will now explore exactly how we arrived at this conclusion.</p>
<p>First, note that this tree exists as a result of the recurrence. We used the
code to specify the recurrence, and then the recurrence itself described this
tree. It has a branching factor of 2 (that is, two children of each node that
are non-leaves) since the recursive case of the recurrence has two recursive
calls, and at each level the size of the input changes. Since the recursive
calls are called on inputs of size \( \frac{n}{2} \), each level results in a
division by two of the input size.</p>
<p>Additionally, we know that the amount of work at each node (of input size \( n \))
is necessarily \( c_1 \frac{n}{2} \). There is technically also a \( c_0 \) term,
but we will omit it since it is asymptotically dominated by \( c_1 \frac{n}{2} \).
The precise non-recursive work done by each &quot;node&quot; is specified slightly down
and to the left of each node. Individually, they don't look very nice to sum
over - at level \( i \), it appears the work at each node is \( c_1
\frac{n}{2^{i+1}} \). However, level \( i \) also has \( 2^i \) nodes, by the
branching nature of the recurrence tree. As such, the total amount of work done
at level \( i \) is just \( c_1 \frac{n}{2^{i+1}} * 2^i = c_1 \frac{n}{2} \), which
is not a function of the level \( i \).</p>
<p>As such, each level has the same amount of work - which is very convenient, as
we can now just take that quantity and multiply it by the number of levels. So
in total, when we solve out the recurrence, we should obtain that \( W(n) =
(\sum_{i=1}^{\log_2(n)} c_1\frac{n}{2}) + c_2n \), since the \( c_2n \) term is
separately obtained from the base level, due to the \( n \) leaves that each have
\( c_2 \) work. </p>
<p>The term \( \sum_{i=1}^{\log_2(n)} c_1\frac{n}{2} \) thus goes to \( \frac{c_1}{2}
n\log_2(n) \), so in total we obtain that \( W(n) = \frac{c_1}{2} n\log_2(n) +
c_2n \), which is in \( O(n \log n) \). So we're done.</p>
<p>The tree method is really nothing more than just a visual way to view the
recurrence - it is possible to achieve the same effect by just writing a
summation. It is sometimes more intuitive to try and visualize, however, and for
recurrences where the levels sum to the same amount, the tree method is very
effective. However, not all recurrences exhibit such behavior, and it's hard to
know <em>a priori</em> whether a given recurrence is such a one. Nevertheless, it is a
powerful method and sufficient for many purposes.</p>
<p>We omit the span analysis of this function for the reader.</p>
<h2 id="conclusions"><a class="header" href="#conclusions">Conclusions</a></h2>
<p>Asymptotic analysis is a very important technique for attempting to categorize the efficiency of programs. Moreover, it is not enough to simply find the asymptotic <em>sequential</em> complexity of a function - parallel computation is becoming increasingly more important, and purely sequential analyses are not representative of real-world algorithms. Work and span analyzed through recurrence relations form a powerful framework for examining the complexity of recursive functions, which is robust enough to classify many kinds of algorithms.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../concepts/analysis.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                            <a rel="next" href="../concepts/datatypes.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../concepts/analysis.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                    <a rel="next" href="../concepts/datatypes.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
    </body>
</html>
